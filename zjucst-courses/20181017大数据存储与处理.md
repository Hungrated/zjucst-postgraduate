20181017大数据存储与处理

MapReduce编程

WordCount

机器学习的基础是统计 要求用概率论的思想解决问题

WordCount：内部如何实现 如何编写MR程序 代码什么流程
三个主要部分：
    Driver 程序执行接口
        预先设置好一些条件
        job.setJarByClass(WordCount.class); 
        job.setMapperClass(TokenizerMapper.class); 
        job.setCombinerClass(IntSumReducer.class); 
        job.setReducerClass(IntSumReducer.class); 
        job.setOutputKeyClass(Text.class); 
        job.setOutputValueClass(IntWritable.class);
        最后等待job的执行
    Mapper
        继承Mapper对象 实现map方法
        遍历文件内所有单词 用StringTokenizer迭代
        注意：这一步只是分解出所有单词，并不做合并
        输入数据类型：Objekt, Text
        输出数据类型：Text, int
    Reducer
        继承Reducer对象 实现reduce方法
        将相同key的值加在一起 产生最后的输出结果

API介绍 几张图
Map过程 通过在输入列表中的每一项执行函数， 生成一系列的输出列表
Reduce过程 在一个输入的列表进行扫描工作，随 后生成一个聚集值，作为最后的输出
MR的Reduce过程 对于含有不同的键值的集合，所有相同键值的列表被输入到同一个Reduce任务中
MapReduce的数据流程和执行过程（两张图）

输入文件InputFile 一般保存在HDFS中 类型不固定 文件经常很大（GB级）
InputFile提供了以下一些功能
– 选择文件或者其它对象，用来作为输入
– 定义InputSplits，将一个文件分开成为任务
– 为RecordReader提供一个工厂，用来读取这个文件

输入格式InputFormat
定义了这些文件如何分割，读取
类型：
TextInputFormat，默认的格式，每一行是一个单独的记录， 并且作为value，文件的偏移值作为key
KeyValueTextInputFormat，这个格式每一行也是一个单独的 记录，但是Key和Value用Tab隔开，是默认的OutputFormat ，可以作为中间结果，作为下一步MapReduce的输入。
SequenceFileInputFormat
– 基于块进行压缩的格式
– 对于几种类型数据的序列化和反序列化操作 – 用来将数据快速读取到Mapper类中

InputSplits
InputSplit定义了输入到单个Map任务的输入数据
一个MapReduce程序被统称为一个Job，可能有上百个 任务构成
InputSplit将文件分为64MB的大小
– hadoop-site.xml中的mapred.min.split.size参数控制 这个大小
mapred.tasktracker.map.taks.maximum用来控制某一 个节点上所有map任务的最大数目

RecordReader
InputSplit定义了一项工作的大小，但是没有定义如何读 取数据
RecordReader实际上定义了如何从数据上转化为一个 (key,value)对，从而输出到Mapper类中
TextInputFormat提供了LineRecordReader

其余操作：Mapper Partition & Shuffle Sort Reduce

OutputFormat
写入到HDFS的所有OutputFormat都继承自 FileOutputFormat
每一个Reducer都写一个文件到一个共同的输出 目录，文件名是part-nnnnn，其中nnnnn是与每 一个reducer相关的一个号(partition id)
FileOutputFormat.setOutputPath() • JobConf.setOutputFormat()

RecordWriter 用来指导如何输出一个记录到文件中
Combiner

容错处理 由Hadoop系统自己解决

案例与高级编程 自己练习

